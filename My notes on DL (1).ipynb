{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload data files\n",
    "<p class=\"lead\">This <a href=\"https://jupyter.org/\">Jupyter notebook</a>\n",
    "shows how to upload data files to be converted\n",
    "to [Photon-HDF5](http://photon-hdf5.org) format. </p>\n",
    "\n",
    "<i>Please send feedback and report any problems to the \n",
    "[Photon-HDF5 google group](https://groups.google.com/forum/#!forum/photon-hdf5).</i>\n",
    "\n",
    "<br>\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>NOTE</b> Uploading data files is only necessary when running the notebook online.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    .warning { color: green; }\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "    .warning { color: green; }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<span class=\"warning\">fff fff ff </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issues you need to consider to build you Deep Neural Networks\n",
    "- $#$ layers\n",
    "- $#$ hidden units\n",
    "- what is the learning rate $\\alpha$\n",
    "- what is the appropriate activation function to be used\n",
    "- $\\beta$ in the momentum algorithm\n",
    "- $\\beta_2$ in the RMSprop algorithm\n",
    "- $\\epsilon$ for the ADAM algorithm and RMSprop algorithm\n",
    "- Decay rate and $\\alpha_0$\n",
    "- Shuffling and Partitioning are the two steps required to build mini-batches, powers of two are often chosen to be the mini-batch size, e.g., 16, 32, 64, 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- you cna't guess such hyperparameters from the first time they need to be calculated in a highly iterative process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Your Data Sets in Terms of Train, Development and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The development set is also called hold-out set and cross-validation set\n",
    "- 70%-30% split and 60%-20%-20% split are consired good old practice in the field of machine learning beause the number of the traing examples are small i.e. 100, 1000, or even 10000\n",
    "- but in the modern big-data era where for example you might have 1,000,000 example in total, then the trend is that your dev and test sets have been becoming a much smaller percentage of the total (for example the split going to be 98%-1%-1%). \n",
    "- The reason for that is that the goal of the dev set is just you're going to test different algorithms on it and see which algorithm works better.\n",
    "- <span class=\"text-danger\">a very important rule of thumb to follow is to make sure that the dev and test sets come from the same distribution.</span> Because you will be using the dev set to evaluate a lot of different models and trying really hard to improve performance on the dev set, it's nice if your dev set comes from the same distribution as your test set.\n",
    "- If the examples is split into only two data sets, we call the first as the training set and the other one is the dev set (Not having a test set might be okay)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.random.randn(100,4)\n",
    "y_true = np.random.randint(2, size=(100,3))\n",
    "\n",
    "#X should be in shape m*n where m is the number of samples and n is the number of features as well as the y_True\n",
    "#the function shuffles the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_true, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias and Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the two key numbers to look at to understand bias and variance will be the train set error and the dev set.\n",
    "<table style=\"width:100%;text-align:left\" align=\"left\">\n",
    "  <tr><td>**Training set error**</td><td>1%</td><td>15%</td><td>15%</td><td>0.5%</td></tr>\n",
    "  <tr><td>**Training set error**</td><td>11%</td><td>16%</td><td>30%</td><td>1%</td></tr>\n",
    "  <tr><td>**Diagnose**</td><td>Overfitting (high variance)</td><td>underfitting (high bias) <br/> (Assuming that human error ~0%)</td><td>high bias + high variance <br/> (Assuming that human error ~0%)</td><td>low bias + low variance</td></tr>\n",
    "</table>\n",
    "- the table above is based on 2 assumtions; (a) human error is about ~ 0%, and (b) training and dev sets are drawn from the same distribution\n",
    "- Human error is sometimes called the optimal error or the Bayes error\n",
    "- If the optimal error is about ~15%, then the second column is prefect (means low bias and low variance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving high Bias and high variance problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The following technique follows the <span class=\"text-danger\">orthognalization principle</span>\n",
    "\n",
    "```python\n",
    "while high_bias (training data performance):\n",
    "    # make sure that Bayes error is not too high\n",
    "    try:\n",
    "        Bigger network\n",
    "        train longer\n",
    "        find new NN architecture\n",
    "\n",
    "while high_variance (dev data performance):\n",
    "    try:\n",
    "        get more data \n",
    "        regularization\n",
    "        find new NN architecture\n",
    "else:\n",
    "    if high_bias (training data performance)\n",
    "    goto: high_bias (training data performance)\n",
    "```\n",
    "\n",
    "- <span class=\"text-danger\">Getting a bigger network</span> almost always just reduces bias without necessarily hurting variance, so long as you regularize appropriately, and <span class=\"text-danger\">getting more data</span> pretty much always reduces variance and doesn't hurt bias much.\n",
    "- So what's really happened is that, with these two steps, the ability to train, pick a network, or get more data, we now have tools to drive down bias and just drive down bias, or drive down variance and just drive down variance, without really hurting the other thing that much. \n",
    "- this has been one of the big reasons that deep learning has been so useful for supervised learning, that there's much less of this tradeoff where you have to carefully balance bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing /Exploding Gradients\n",
    "\n",
    "- One of the problems of training neural network, especially very deep neural networks, that is vanishing and exploding gradients.\n",
    "- when you're training a very deep network your derivatives or your slopes can sometimes get either very, very big or very, very small, maybe even exponentially small, and this makes training difficult.\n",
    "- careful choice of how you initialize the weights solve the problem of vanishing/exploding gradients **partially**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REGULATIZING YOUR NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As $\\lambda$ gets large as the model has less overfitting \n",
    "- when $\\lambda = 0$ the regularization term is neglected\n",
    "- the regularization reduce the complexity of the entire network by making $g^{[l]}(Z^{[l]})$ getting more linear, and when that happens the entire network tends to be linear i.e. ~ $= X W^{[1]}W^{[2]}W^{[3]} ... W^{[L]}$\n",
    "- <span style=\"color:green\">when drawing the curve of the cost function as a function of no. of iteration don't forget to add the regularization term in both the cost function and the gradients.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different Type of Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) $L_2$ regularization formula is as follows:\n",
    "\n",
    "The cross-entropy cost $J(w, b) =$\n",
    "$$ \\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\mathcal{L}(a^{[L](i)}, y^{(i)}) + \\frac{\\lambda}{2m} \\lVert w \\rVert^{2}_2 = $$\n",
    "\n",
    "$$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) + \\frac{\\lambda}{2m} \\lVert w \\rVert^{2}_2 $$\n",
    "\n",
    "- Where: $\\lVert w \\rVert^{2}_2 = \\sum\\limits_{j = 1}^{n_x} w^2_j$ is the Euclidean norm or $L_2$ norm\n",
    "\n",
    "- Lambda here is called the <span style=\"color:red\">Regularization Parameter</span>.\n",
    "- The dev set is used to set the regularization parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) $L_1$ regularization\n",
    "\n",
    "We add the term $ \\frac{\\lambda}{2m} \\lVert w \\rVert_1$\n",
    "\n",
    "where Where: $\\lVert w \\rVert_1 = \\sum\\limits_{j = 1}^{n_x} \\lvert w_j\\rvert$ is the Euclidean norm or $L_2$ norm\n",
    "\n",
    "- If you use $L_1$ regularization, then w will end up being sparse i.e. the w vector will have a lot of zeros. \n",
    "- Using $L_1$ regularization can help with compressing the model, because the set of parameters are zero, and you need less memory to store the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Regularization of Neural Network:\n",
    "- it is a special case of $L_2$ regularization, and it is some times called <span style=\"color:red\">weight decay</span>\n",
    "$$ \\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\mathcal{L}(a^{[L](i)}, y^{(i)}) + \\frac{\\lambda}{2m} \\sum\\limits_{l = 1}^{L} \\lVert w^{[l]} \\rVert^{2}_F$$\n",
    "\n",
    "The term $\\lVert w^{[l]} \\rVert^{2}_F$ is called <span style=\"color:red\">frobenius norm</span>\n",
    "$$\\lVert w^{[l]} \\rVert^{2}_F = \\sum\\limits_{i = 1}^{n^{l-1}}\\sum\\limits_{j = 1}^{n^{l}}w^{[l]}_{ij} $$\n",
    "\n",
    "- The regularization term also need to be added to the gradient components by adding the term $\\frac{\\lambda}{m} w^{[l]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With dropout, what happens is that we go through each of the layers of the network and set some probability of eliminating a node in neural network.\n",
    "- and in the learning phase, for each training example in the dataset we eliminate different set of nodes from each liear.\n",
    "- Because you're training a much smaller network on each example it gives a sense for why you end up able to regularize the network,\n",
    "- <span class=\"alert-success\">the most common way to implement dropout is called **Inverted dropout**</span>\n",
    "\n",
    "```python\n",
    "# keep_prob represent the number of node to keep in layer 3\n",
    "d3 = np.random.rand(*a3.shape)<keep_prob\n",
    "a3 = np.multiply(a3,d3)\n",
    "\n",
    "# the inverted dropout technique\n",
    "a3 /= keep_prob\n",
    "```\n",
    "- In order to not reduce the expected value of $z^{[4]}$, what you do is you need to take $A^{[3]}$, and divide it by *keep_prob* value because this will correct or just a bump that back up. So it's not changed the expected value of $A^{[3]}$. \n",
    "\n",
    "- And its effect is that, no matter what you set to keep.prob to, whether it's 0.8 or 0.9 or even one,  or 0.5 or whatever, this inverted dropout technique by dividing by the keep_prob, it ensures that the expected value of a3 remains the same\n",
    "\n",
    "- if keep_prob value is set to one then there's no dropout, because it's keeping everything\n",
    "- <span class=\"text-info\">Notice that, in the test time don't use dropout explicitly, because when you are making predictions at the test time, you don't really want your output to be random. If you are implementing dropout at test time, that just add noise to your predictions.</span>\n",
    "- the intuition of drop-out is that \"it can't rely on any one feature, so have to spread out weights\"\n",
    "- similar to what we saw with L2 regularization, the effect of implementing drop out is that it shrinks the weights that helps prevent over-fitting.\n",
    "- <span class='alert-success'>The *keep_prob* value differ for each layer in the network, give *keep_prob* a heigher value when you less worried about overfiiting. i.e. layers with big number of hidden units give it a small value</span>\n",
    "\n",
    "- Technically, you can also apply drop-out to the input layer. Although in practice, we usually don't do that often. So, a key prop of 1.0 was quite common for the input layer.\n",
    "\n",
    "- One big downside of drop-out is that the cost function $J$ is no longer well-defined.\n",
    "- <span class=\"text-info\">On every iteration, you are randomly killing off a bunch of nodes. And so, if you are double checking the **performance of gradient descent**, it's actually harder to double check that you have a well defined cost function J that is going downhill on every iteration, because the cost function J that you're optimizing is actually less well-defined, or is certainly hard to calculate. So you lose this debugging tool. </span>\n",
    "- <span class=\"text-info\">So what should usually happen is turn off drop out, you will set key prop equals one, and I run my code and make sure that it is monotonically decreasing J, and then turn on drop out and hope that I didn't introduce bugs into my code during drop out.</span>\n",
    "\n",
    "- <span class=\"text-danger\">Gradient check</span> does not work with drop-out.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Methods for Reducing Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span class=\"text-danger\">Data augmentation</span> is a method that you can use to increase the size of your dataset by adding the same data with manipulated in some way.\n",
    "- Data augmentation is not the same as collecting a new data but it elemenate the difficulties to getting new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span class=\"text-danger\">Early stopping</span> is the process of train the model with the same number of iterations where the error of dev set is the minimum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $L_2$ vs. early stopping for solving the high variance problem\n",
    "\n",
    "- Early stopping eliminate the orthognalization principle from solving the problems of training a model and reducing overfitting\n",
    "- The main downside of early stopping is that the two tasks; optimizing cost function $J$ and not to overfit, no longer can work on these two problems independently. Because by stopping gradient decent early, you've sort of not done that well and then also simultaneously trying to not over fit. \n",
    "\n",
    "- Using early stopping  makes the set of things you could try are more complicated to think about, because instead of using different tools to solve the two problems, you're using one that kind of mixes the two.\n",
    "\n",
    "- Rather than using early stopping, one alternative is just use $L_2$ regularization. In such case you can just train the neural network as long as possible. \n",
    "\n",
    "- Using $L_2$ instead of early stopping makes the search space of hyperparameters easier to decompose, and easier to search over. \n",
    "\n",
    "- The downside of Using $L_2$ is that you might have to try a lot of values of the regularization parameter lambda and that is more computationally expensive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up the optimization problem\n",
    "\n",
    "1. Feature normalization\n",
    "2. Cross-validation\n",
    "3. Randomly initialize weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Mapping (AKA Feature Normalization) \n",
    "- speed up your training\n",
    "- always use the same $\\mu$ and $\\sigma$ for feature normalization\n",
    "- doing input normalization does not harm the performance even if your features in a relatively same scale.\n",
    "\n",
    "### METHOD 1 \n",
    "```python \n",
    "def features_normalize(features):\n",
    "    if features.ndim == 2:\n",
    "        col_count = features.shape[1]\n",
    "    \n",
    "    mu = np.mean(features, axis=0)\n",
    "    features_norm = features-mu\n",
    "    sigma = np.std(features_norm, axis=0, ddof=1)\n",
    "    \n",
    "    return np.divide(features_norm, sigma), mu, sigma\n",
    "```\n",
    "\n",
    "### METHOD 2  via Scikit-learn\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "fts = np.random.randint(1000, size=(4,2)) #  examples X features\n",
    "print(fts)\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "print(mms.fit_transform(fts))\n",
    "\n",
    "ss = StandardScaler()      # as andrew's mean=0 and std = 1\n",
    "print(ss.fit_transform(fts))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cross-validation \n",
    "### METHOD 1\n",
    "\n",
    "```python\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "def build_nn_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(25, input_shape=(400,), activation='sigmoid')) # sigmoid is better than tanh I don't know why\n",
    "    # model.add(Dense(2, activation='tanh'))\n",
    "    model.add(Dense(10, activation='softmax'))  # try sigmoid and softmax\n",
    "    model.compile(Adam(lr=0.05), 'categorical_crossentropy', metrics=['accuracy'])  # binary_crossentropy\n",
    "    return model\n",
    "\n",
    "cv_model = KerasClassifier(build_fn=build_nn_model, epochs=30, verbose=0)\n",
    "cv = KFold(5,shuffle=True)  # defining a 5-fold cross validation \n",
    "scores = cross_val_score(cv_model, features, incidence_y, cv=cv)\n",
    "\n",
    "print(scores)  # accuracies\n",
    "print(scores.mean())\n",
    "print(scores.std())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Randomly initialize weights\n",
    "### METHOD 1\n",
    "```python\n",
    "def rand_initialize_weights(L_in_size, L_out_size):\n",
    "    '''\n",
    "    L_in_size = the number of units in the previous layer\n",
    "    L_out_size = the number of units in the next layer\n",
    "    '''\n",
    "    epsilon_init = np.sqrt(6)/np.sqrt(L_in_size+L_out_size)\n",
    "    epsilon_init = 0.12\n",
    "    return  rand(L_out_size, L_in_size+1) * 2*epsilon_init - epsilon_init\n",
    "```\n",
    "\n",
    "### METHOD 2\n",
    "``` python\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        ### START CODE HERE ### (â‰ˆ 2 lines of code)\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])* 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))*0.01\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "    return parameters\n",
    "```\n",
    "### METHOD 3 Keras\n",
    "```python\n",
    "initializers = ['zeros', 'uniform', 'normal',\n",
    "                'he_normal', 'lecun_uniform']\n",
    "                \n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(4,),\n",
    "                    kernel_initializer=initializers[0],\n",
    "                    activation='sigmoid'))\n",
    "                    \n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "h = model.fit(X_train, y_train, batch_size=16, epochs=5, verbose=0)\n",
    "```\n",
    "### METHOD 4 (for the problem of vanishing/exploding gradients )\n",
    "\n",
    "try <span class=\"text-danger\">\"He Initialization\"</span>; this is named for the first author of He et al., 2015. (If you have heard of <span class=\"text-danger\">\"Xavier initialization\"</span>, this is similar except Xavier initialization uses a scaling factor for the weights $W^{[l]}$ of `sqrt(1./layers_dims[l-1])` where He initialization would use `sqrt(2./layers_dims[l-1])`.)\n",
    "\n",
    "```python\n",
    "if activation_function == 'relu':\n",
    "    # He Initialization\n",
    "    W = np.random.rand(*shape) * np.sqrt(2/n_of_the_last_layer)\n",
    "else:\n",
    "    # Xavier initialization\n",
    "    W = np.random.rand(*shape) * np.sqrt(1/n_of_the_last_layer)\n",
    " \n",
    "# benjio\n",
    "W = np.random.rand(*shape) * np.sqrt(2/(n_of_the_last_layer+n_of_this_layer))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
