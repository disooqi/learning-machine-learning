{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$n = n_x =$ number of input features\n",
    "\n",
    "$m =$ number of training examples\n",
    "\n",
    "$(x, y)$, where $x\\in \\mathbb{R}^{n_x}$\n",
    "\n",
    "X is n * m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples, m = 100\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "file_path = 'data/ex2data1.txt'\n",
    "data01 = np.loadtxt(file_path, delimiter=',')\n",
    "\n",
    "m, _ = data01.shape\n",
    "X = data01[:,:-1].T\n",
    "Y = data01[:,-1][np.newaxis, :]\n",
    "print('number of training examples, m = {}'.format(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y, a):\n",
    "    return -(y*np.log(a) + (1-y)*np.log(1-a))\n",
    "\n",
    "def cost_function(W, b):\n",
    "    #agg = 0\n",
    "    #for training_example in X.squeeze():\n",
    "    #    z = np.dot(W, training_example) + b\n",
    "    #    agg += loss_function(training_example, y_hat(z))\n",
    "    #else:\n",
    "    #    print(agg/m)\n",
    "    \n",
    "    Z = np.dot(W.T, X) + b\n",
    "    return np.sum(loss_function(Y, y_hat(Z)))/m\n",
    "        \n",
    "def y_hat(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931471805599453"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize W and b\n",
    "initial_W = np.zeros((X.shape[0], 1))\n",
    "initial_b = 0\n",
    "# np.log(y_hat(z = np.dot(W, 17.44) + b))\n",
    "cost_function(W, b) # [[0.69314718]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-12.00921659]\n",
      " [-11.26284221]] [[-0.1]]\n"
     ]
    }
   ],
   "source": [
    "# dW = dZdW * dAdZ * dLdA * dJdL\n",
    "\n",
    "# if Z = np.dot(W.T, X) + b\n",
    "# dZdW = X\n",
    "\n",
    "# if A is sigmoid function\n",
    "# dAdZ = A(1-A)\n",
    "\n",
    "# if L is the cross entropy error\n",
    "# dLdA = -Y/A + (1-Y)/(1-A)\n",
    "\n",
    "# dLdW = dZdW*dAdZ*dLdA\n",
    "# if J = average of all example losses\n",
    "# dJdL = np.sum(dLdW)/m\n",
    "\n",
    "\n",
    "def calculate_gradients(W, b, X, Y):\n",
    "    dZdW = X # --------------------------------------- (1)\n",
    "    dZdb = 1 # --------------------------------------- (1)\n",
    "    \n",
    "    Z = np.dot(W.T, X) + b\n",
    "    A = y_hat(Z)\n",
    "    dAdZ = A*(1-A) # ----------------------------------(2)\n",
    "    \n",
    "    dLdA = -Y/A + (1-Y)/(1-A) # ---------------------- (3)\n",
    "    \n",
    "    dLdW = dZdW * dAdZ * dLdA # ---------------------- (4)\n",
    "    dLdb = dZdb * dAdZ * dLdA # ---------------------- (4)\n",
    "    \n",
    "    #dW or shortly \"np.dot(X,(A-Y).T)/m\" and db or shortly \"np.sum((A-Y).T)/m\"\n",
    "    dJdW = np.sum(dLdW, axis=1, keepdims=True)/m # --- (5)\n",
    "    dJdb = np.sum(dLdb, axis=1, keepdims=True)/m # --- (5)\n",
    "    \n",
    "    return dJdW, dJdb\n",
    "\n",
    "dW, db = calculate_gradients(initial_W, initial_b, X, Y)\n",
    "print(dW, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of J before learning is 0.6931471805599453\n",
      "[[0.04528064]\n",
      " [0.03819149]] [[-4.81180027]]\n",
      "The value of J after learning is 0.38738952271118804\n"
     ]
    }
   ],
   "source": [
    "def update_weights(W, b, X_data, target, alpha=0.001, iterations=100000):\n",
    "    print(\"The value of J before learning is\", cost_function(W, b))\n",
    "    for i in range(iterations):\n",
    "        dW, db = calculate_gradients(W, b, X_data, target)\n",
    "        W = W - alpha*dW\n",
    "        b = b - alpha*db\n",
    "        # print(\"The value of J after learning is\", cost_function(W, b))\n",
    "    else:\n",
    "        print(W, b)\n",
    "        print(\"The value of J after learning is\", cost_function(W, b))\n",
    "\n",
    "update_weights(initial_W, initial_b, X, Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
