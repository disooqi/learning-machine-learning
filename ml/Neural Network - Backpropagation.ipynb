{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array, arange, cos, exp, pi, zeros, column_stack, ones, newaxis, log, dot, append, zeros_like\n",
    "from numpy.random import permutation, shuffle, random, randint, rand\n",
    "from scipy.io import loadmat\n",
    "from scipy.optimize import minimize, fmin_bfgs\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "from IPython.display import Latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup the parameters you will use for this exercise\n",
    "input_layer_size  = 400;  # 20x20 Input Images of Digits\n",
    "hidden_layer_size = 25;   # 25 hidden units\n",
    "num_labels = 10;          # 10 labels, from 1 to 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### =========== Part 1: Loading and Visualizing Data ============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 400), (5000, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handwritten_digits = loadmat('ex4data1.mat')\n",
    "handwritten_digits.keys()\n",
    "\n",
    "features = handwritten_digits['X']\n",
    "m, n = features.shape\n",
    "\n",
    "org_y = handwritten_digits['y']\n",
    "y = org_y.copy()\n",
    "y[y==10] = 0\n",
    "features.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ================ Part 2: Loading Parameters ================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', 'Theta1', '__globals__', 'Theta2'])\n",
      "(25, 401) (10, 26)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10285,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading Saved Neural Network Parameters ...\n",
    "weight = loadmat('ex3weights.mat')\n",
    "print(weight.keys())\n",
    "\n",
    "# Unroll parameters \n",
    "t1 = weight['Theta1'].ravel(order='F')\n",
    "t2 = weight['Theta2'].ravel(order='F')\n",
    "# nn_params = [Theta1(:) ; Theta2(:)];\n",
    "print(weight['Theta1'].shape, weight['Theta2'].shape)\n",
    "nn_params = np.r_[t1, t2]\n",
    "nn_params.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ================ Part 3: Compute Cost (Feedforward) ================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at parameters (loaded from ex4weights):  0.287629165161   \n",
      "(this value should be about 0.287629)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_lambda = 0\n",
    "J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, features, y, _lambda)\n",
    "print ('Cost at parameters (loaded from ex4weights): ', J,'  \\n(this value should be about 0.287629)\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ================== Part 4: Implement Regularization ==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at parameters (loaded from ex4weights):  0.383769859091   \n",
      "(this value should be about 0.383770)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_lambda = 1\n",
    "J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, features, y, _lambda)\n",
    "print('Cost at parameters (loaded from ex4weights): ', J,'  \\n(this value should be about 0.383770)\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ================ Part 5: Sigmoid Gradient  ================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid gradient evaluated at [1 -0.5 0 0.5 1]:\n",
      "  \n",
      "[ 0.19661193  0.23500371  0.25        0.23500371  0.19661193]\n"
     ]
    }
   ],
   "source": [
    "def sigmoidGradient(z):\n",
    "    return sigmoid(z) * (1-sigmoid(z))\n",
    "\n",
    "print('Sigmoid gradient evaluated at [1 -0.5 0 0.5 1]:\\n  ')\n",
    "print(sigmoidGradient(array([1, -0.5, 0, 0.5, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ================ Part 6: Initializing Pameters ================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### =============== Part 7: Implement Backpropagation ==============="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps for training a Neural Network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Randomly initialize weights\n",
    "\n",
    "We usually initialize the weights to small values close to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 401) (10, 26)\n"
     ]
    }
   ],
   "source": [
    "def randInitializeWeights(L_in_size, L_out_size):\n",
    "    epsilon_init = np.sqrt(6)/np.sqrt(L_in_size+L_out_size)\n",
    "    epsilon_init = 0.12\n",
    "    return  rand(L_out_size, L_in_size+1) * 2*epsilon_init - epsilon_init\n",
    "\n",
    "\n",
    "initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size);\n",
    "initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels);\n",
    "\n",
    "print(initial_Theta1.shape, initial_Theta2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Implement forward propagation to get $h_\\Theta(x^{(i)})$ for any $x^{(i)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feed_forward(x, Theta1, Theta2):\n",
    "    z2 = Theta1.dot(x[:,newaxis])\n",
    "    a2 = np.r_[[[1]], sigmoid(z2)]\n",
    "    \n",
    "    z3 = Theta2.dot(a2)\n",
    "    return sigmoid(z3).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Implement code to compute cost function $J(\\Theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z): \n",
    "    return 1/(1+exp(-z))\n",
    "\n",
    "def sigmoidGradient(z):\n",
    "    return sigmoid(z) * (1-sigmoid(z))\n",
    "\n",
    "def nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, features, y, reg_parameter):\n",
    "    if nn_params.ndim != 1:\n",
    "        return\n",
    "    theta1_size = (input_layer_size+1) * hidden_layer_size\n",
    "    Theta1 = nn_params[:theta1_size].reshape((hidden_layer_size,input_layer_size+1), order='F') # (25, 401)\n",
    "    Theta2 = nn_params[theta1_size:].reshape((num_labels, hidden_layer_size+1), order='F') # (10, 26)\n",
    "    \n",
    "    m, _ = features.shape\n",
    "    a_1 = np.c_[ones((m)), features]\n",
    "    \n",
    "    z_2 = Theta1.dot(a_1.T) # (25, 401) * (401, 5000)\n",
    "    a_tmp = sigmoid(z_2)    # (25, 5000)\n",
    "    \n",
    "    a_2 = np.vstack((ones((m)), a_tmp))\n",
    "    z_3 = Theta2.dot(a_2)\n",
    "    a_3 = sigmoid(z_3)\n",
    "    \n",
    "    #ex_sum = 0\n",
    "    #for i in arange(m):\n",
    "    #    yVec = zeros((num_labels,1))\n",
    "    #    yVec[y[i]] = 1\n",
    "    #    yVec = yVec.ravel()\n",
    "    #    yVec = np.roll(yVec, -1)\n",
    "    #    ex_sum = ex_sum+ np.sum(-yVec*np.log(a_3[:,i]) - (1-yVec)*np.log(1 - a_3[:,i]))\n",
    "    #else:\n",
    "    #    print ex_sum/m\n",
    "    \n",
    "    incidence_y = zeros((y.size, num_labels))\n",
    "    y_1 = y.ravel()\n",
    "    \n",
    "    incidence_y[arange(m), y_1] = 1  # (5000, 10)\n",
    "    incidence_y = np.roll(incidence_y, -1, axis=1)\n",
    "    \n",
    "    reg_term = _lambda *(np.sum(Theta1[:,1:]**2) + np.sum(Theta2[:,1:]**2))/(2*m)\n",
    "    \n",
    "    return np.sum(-incidence_y*np.log(a_3.T) - (1-incidence_y)*np.log(1 - a_3.T))/m +reg_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- Implement backprop to compute partial derivatives $\\frac{\\partial}{\\partial \\Theta_{jk}^{(l)}} J(\\Theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nn_gradient(nn_params, input_layer_size, hidden_layer_size, num_labels,features, y, _lambda):\n",
    "    m = y.size\n",
    "    X = np.c_[ones((m)), features]\n",
    "    \n",
    "    \n",
    "    \n",
    "    if nn_params.ndim != 1:\n",
    "        return\n",
    "    theta1_size = (input_layer_size+1) * hidden_layer_size\n",
    "    Theta1 = nn_params[:theta1_size].reshape((hidden_layer_size,input_layer_size+1), order='F') # (25, 401)\n",
    "    Theta2 = nn_params[theta1_size:].reshape((num_labels, hidden_layer_size+1), order='F') # (10, 26)\n",
    "\n",
    "    Delta2 = zeros_like(Theta2)\n",
    "    Delta1 = zeros_like(Theta1)\n",
    "\n",
    "    for i in arange(m):\n",
    "        \n",
    "        # forward pass\n",
    "        x = X[i,:]\n",
    "    \n",
    "        z2 = Theta1.dot(x[:,newaxis])\n",
    "        a2 = np.r_[[[1]], sigmoid(z2)]\n",
    "    \n",
    "        z3 = Theta2.dot(a2)\n",
    "        hx = sigmoid(z3).ravel()\n",
    "    \n",
    "        # computing the \"error terms\" that measure how much the nodes were responsible for any errors \n",
    "        # in our output\n",
    "        delta3 = hx - incidence_y[i,:]\n",
    "        delta2 = Theta2.T.dot(delta3)[1:] * sigmoidGradient(z2).ravel()\n",
    "    \n",
    "        Delta2 = Delta2 + delta3[:,newaxis].dot(a2.T)\n",
    "        Delta1 = Delta1 + delta2[:,newaxis].dot(x[:,newaxis].T)\n",
    "    \n",
    "    else:\n",
    "        D2 = Delta2/m + _lambda/m * np.c_[zeros((Theta2.shape[0])), Theta2[:,1:]]\n",
    "        D1 = Delta1/m + _lambda/m * np.c_[zeros((Theta1.shape[0])), Theta1[:,1:]]\n",
    "        return np.r_[D1.ravel(order='F'), D2.ravel(order='F')]\n",
    "\n",
    "initial_weights = np.r_[initial_Theta1.ravel(order='F'), initial_Theta2.ravel(order='F')]\n",
    "D = nn_gradient(initial_weights, input_layer_size, hidden_layer_size, num_labels,features, y, _lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $ \\Delta^{(l)} := \\Delta^{(l)} + \\delta^{(l+1)} (a^{(l)})^T $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5- Use gradient checking to compare $\\frac{\\partial}{\\partial \\Theta_{jk}^{(l)}} J(\\Theta)$ computed using packpropagation vs. using numerical estimate of gradient of $J(\\Theta)$ .\n",
    "Then disable gradient checking code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "def gradient_checking(nn_params):\n",
    "    epsilon = 1e-4\n",
    "    grad_vect = zeros_like(nn_params)\n",
    "    for i in arange(nn_params.size):\n",
    "        e_vector = zeros_like(nn_params)\n",
    "        \n",
    "        e_vector[i] = epsilon\n",
    "        \n",
    "        plus = nnCostFunction(nn_params+e_vector,input_layer_size, hidden_layer_size, num_labels, features, y, 0)\n",
    "        minus = nnCostFunction(nn_params-e_vector,input_layer_size, hidden_layer_size, num_labels, features, y, 0)\n",
    "        grad_estimation = (plus - minus)/(2*epsilon)\n",
    "        grad_vect[i] = grad_estimation\n",
    "        \n",
    "        if i%1000 == 0:\n",
    "            print(i)\n",
    "    else:\n",
    "        return grad_vect\n",
    "#         print grad_estimation\n",
    "        \n",
    "#         if i > 100:\n",
    "#             print grad_vect\n",
    "#             break\n",
    "    \n",
    "    \n",
    "G = gradient_checking(initial_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01089709  0.01089709]\n",
      " [-0.00978536 -0.00978536]\n",
      " [ 0.016502    0.016502  ]\n",
      " ..., \n",
      " [ 0.22879093  0.22879093]\n",
      " [ 0.25669864  0.25669864]\n",
      " [ 0.25704499  0.25704499]]\n",
      "Andrew Ng says that you should see a relative difference that is less than 1e-9, but you got: \n",
      "1e-10\n"
     ]
    }
   ],
   "source": [
    "print(np.c_[D,G][10:])\n",
    "\n",
    "for diff_value in [1e-10,1e-9,1e-8,1e-7,1e-6,1e-5,1e-4,1e-3,1e-2,1e-1,1]:\n",
    "    if np.all(np.abs(D-G)<diff_value):\n",
    "        print ('Andrew Ng says that you should see a relative difference that is less than 1e-9, but you got: ',) \n",
    "        print (diff_value)\n",
    "        break\n",
    "    else:\n",
    "        \"something wrong\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6- Use gradient descent or advanced optimization menthod with backpropagation to try to minimize $J(\\Theta)$ as a function of parameters $ \\Theta $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "method : str or callable, optional\n",
    "Type of solver. Should be one of\n",
    "    ‘Nelder-Mead’ (see here)\n",
    "    ‘Powell’ (see here)\n",
    "    ‘CG’ (see here)\n",
    "    ‘BFGS’ (see here)\n",
    "    ‘Newton-CG’ (see here)\n",
    "    ‘L-BFGS-B’ (see here)\n",
    "    ‘TNC’ (see here)\n",
    "    ‘COBYLA’ (see here)\n",
    "    ‘SLSQP’ (see here)\n",
    "    ‘dogleg’ (see here)\n",
    "    ‘trust-ncg’ (see here)\n",
    "    custom - a callable object (added in version 0.14.0), see below for description.\n",
    "'''\n",
    "_lambda = 10\n",
    "# initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size);\n",
    "# initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels);\n",
    "\n",
    "\n",
    "initial_weights = np.r_[initial_Theta1.ravel(order='F'), initial_Theta2.ravel(order='F')]\n",
    "res = minimize(fun=nnCostFunction, x0 =initial_weights, \n",
    "               args=(input_layer_size, hidden_layer_size, num_labels,features, y, _lambda), method='CG', \n",
    "               jac=nn_gradient, options={'maxiter':30})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92.579999999999998"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta1_size = (input_layer_size+1) * hidden_layer_size\n",
    "opt_Theta1 = res.x[:theta1_size].reshape((hidden_layer_size,input_layer_size+1), order='F') # (25, 401)\n",
    "opt_Theta2 = res.x[theta1_size:].reshape((num_labels, hidden_layer_size+1), order='F') # (10, 26)\n",
    "\n",
    "def predict_from_three_layer_NN(Theta1, Theta2, X):\n",
    "    m, _ = X.shape\n",
    "    A_1 = np.c_[ones((m)), X] # (5000, 400)\n",
    "    \n",
    "    Z_2 = Theta1.dot(A_1.T) # (25, 401) * (401, 5000)\n",
    "    A_tmp = sigmoid(Z_2).T # (5000, 25)    \n",
    "    A_2 = np.c_[(ones((m)), A_tmp)] # (5000, 26) \n",
    "    \n",
    "    Z_3 = Theta2.dot(A_2.T) # (10, 26) * (26, 5000) \n",
    "    A_3 = sigmoid(Z_3).T # (5000, 10)\n",
    "    \n",
    "    return A_3\n",
    "\n",
    "pred = predict_from_three_layer_NN(opt_Theta1, opt_Theta2, features)\n",
    "np.mean(pred.argmax(axis=1)+1 == org_y.ravel())*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks via Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 400)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ..., \n",
       "       [9],\n",
       "       [9],\n",
       "       [9]], dtype=uint8)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(features.shape)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_131 (Dense)            (None, 25)                10025     \n",
      "_________________________________________________________________\n",
      "dense_132 (Dense)            (None, 10)                260       \n",
      "=================================================================\n",
      "Total params: 10,285\n",
      "Trainable params: 10,285\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# model.add(Dense(4, input_shape=(2,), activation='tanh'))\n",
    "model.add(Dense(25, input_shape=(400,), activation='sigmoid'))\n",
    "# model.add(Dense(2, activation='tanh'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(Adam(lr=0.05), 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following few lines convert the target vector into an incidence matrix (IN TWO WAYS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "incidence_y = zeros((y.size, num_labels))\n",
    "y_1 = y.ravel()\n",
    "   \n",
    "incidence_y[arange(m), y_1] = 1  # (5000, 10)\n",
    "# incidence_y = np.roll(incidence_y, -1, axis=1)\n",
    "\n",
    "## The following method use Keras to do the same job\n",
    "from keras.utils.np_utils import to_categorical\n",
    "incidence_y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096/5000 [=======================>......] - ETA: 0s[0.0037235659333663533, 0.99903999671936039]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 9, 9, 9])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(features, incidence_y, epochs=30, verbose=0) # also you could use validation_split=0.1 for development\n",
    "print(model.evaluate(features,incidence_y))\n",
    "y_pred = model.predict(features)\n",
    "np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       500\n",
      "          1       1.00      1.00      1.00       500\n",
      "          2       1.00      1.00      1.00       500\n",
      "          3       0.99      0.99      0.99       500\n",
      "          4       0.99      1.00      1.00       500\n",
      "          5       0.98      1.00      0.99       500\n",
      "          6       1.00      1.00      1.00       500\n",
      "          7       1.00      1.00      1.00       500\n",
      "          8       1.00      0.99      1.00       500\n",
      "          9       0.99      0.99      0.99       500\n",
      "\n",
      "avg / total       1.00      1.00      1.00      5000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/disooqi/anaconda2/envs/ztdl/lib/python3.5/site-packages/sklearn/metrics/classification.py:248: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if np.all([l not in y_true for l in labels]):\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "At least one label specified must be in y_true",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-fc7dc666777a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'3'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'4'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'6'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'7'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'9'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda2/envs/ztdl/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"At least one label specified must be in y_true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: At least one label specified must be in y_true"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(y, np.argmax(y_pred, axis=1)))\n",
    "print(confusion_matrix(y, np.argmax(y_pred, axis=1), ['1', '2', '3', '4', '5', '6', '7', '8', '9', '0']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, incidence_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "4000/4000 [==============================] - 0s - loss: 0.0271 - acc: 0.9911     \n",
      "Epoch 2/30\n",
      "4000/4000 [==============================] - 0s - loss: 0.0271 - acc: 0.9911     \n",
      "Epoch 3/30\n",
      "4000/4000 [==============================] - 0s - loss: 0.0244 - acc: 0.9920     \n",
      "Epoch 4/30\n",
      "4000/4000 [==============================] - 0s - loss: 0.0206 - acc: 0.9938     \n",
      "Epoch 5/30\n",
      "4000/4000 [==============================] - 0s - loss: 0.0219 - acc: 0.9933     \n",
      "Epoch 6/30\n",
      "4000/4000 [==============================] - 0s - loss: 0.0199 - acc: 0.9934     \n",
      "Epoch 7/30\n",
      "4000/4000 [==============================] - 0s - loss: 0.0205 - acc: 0.9937     \n",
      "Epoch 8/30\n",
      "4000/4000 [==============================] - 0s - loss: 0.0224 - acc: 0.9931     \n",
      "Epoch 9/30\n",
      "4000/4000 [==============================] - 0s - loss: 0.0221 - acc: 0.9934     \n",
      "Epoch 10/30\n",
      "4000/4000 [==============================] - 0s - loss: 0.0212 - acc: 0.9929     \n",
      "Epoch 11/30\n",
      "4000/4000 [==============================] - 0s - loss: 0.0197 - acc: 0.9937     \n",
      "Epoch 12/30\n",
      "4000/4000 [==============================] - 0s - loss: 0.0208 - acc: 0.9938     \n",
      "Epoch 13/30\n",
      "4000/4000 [==============================] - 0s - loss: 0.0197 - acc: 0.9938     \n",
      "Epoch 14/30\n",
      "4000/4000 [==============================] - 0s - loss: 0.0172 - acc: 0.9945     \n",
      "Epoch 15/30\n",
      "4000/4000 [==============================] - 0s - loss: 0.0185 - acc: 0.9940     \n",
      "Epoch 16/30\n",
      "4000/4000 [==============================] - 0s - loss: 0.0247 - acc: 0.9921     \n",
      "Epoch 17/30\n",
      "4000/4000 [==============================] - 0s - loss: 0.0208 - acc: 0.9933     \n",
      "Epoch 18/30\n",
      "4000/4000 [==============================] - 0s - loss: 0.0177 - acc: 0.9943     \n",
      "Epoch 19/30\n",
      "4000/4000 [==============================] - 0s - loss: 0.0151 - acc: 0.9955     \n",
      "Epoch 20/30\n",
      "4000/4000 [==============================] - 0s - loss: 0.0148 - acc: 0.9951     \n",
      "Epoch 21/30\n",
      "4000/4000 [==============================] - 0s - loss: 0.0146 - acc: 0.9953     \n",
      "Epoch 22/30\n",
      "4000/4000 [==============================] - 0s - loss: 0.0141 - acc: 0.9964     \n",
      "Epoch 23/30\n",
      "4000/4000 [==============================] - 0s - loss: 0.0147 - acc: 0.9956     \n",
      "Epoch 24/30\n",
      "4000/4000 [==============================] - 0s - loss: 0.0123 - acc: 0.9964     \n",
      "Epoch 25/30\n",
      "4000/4000 [==============================] - 0s - loss: 0.0146 - acc: 0.9953     \n",
      "Epoch 26/30\n",
      "4000/4000 [==============================] - 0s - loss: 0.0127 - acc: 0.9962     \n",
      "Epoch 27/30\n",
      "4000/4000 [==============================] - 0s - loss: 0.0178 - acc: 0.9942     \n",
      "Epoch 28/30\n",
      "4000/4000 [==============================] - 0s - loss: 0.0250 - acc: 0.9926     \n",
      "Epoch 29/30\n",
      "4000/4000 [==============================] - 0s - loss: 0.0244 - acc: 0.9925     \n",
      "Epoch 30/30\n",
      "4000/4000 [==============================] - 0s - loss: 0.0225 - acc: 0.9929     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff94005ef28>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=30)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  32/1000 [..............................] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.05270997177809477, 0.98690000724792482]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)\n",
    "# model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN with Cross-validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_nn_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(25, input_shape=(400,), activation='sigmoid')) # sigmoid is better than tanh I don't know why\n",
    "    # model.add(Dense(2, activation='tanh'))\n",
    "    model.add(Dense(10, activation='softmax'))  # try sigmoid and softmax\n",
    "    model.compile(Adam(lr=0.05), 'categorical_crossentropy', metrics=['accuracy'])  # binary_crossentropy\n",
    "    return model\n",
    "\n",
    "cv_model = KerasClassifier(build_fn=build_nn_model, epochs=30, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-89-a70d2c35ac11>, line 3)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-89-a70d2c35ac11>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    scores = cross_val_score(cv_model, features, incidence_y, cv=cv, fit_params=[validation_split=0.1])\u001b[0m\n\u001b[0m                                                                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "cv = KFold(5,shuffle=True)  # defining a 5-fold cross validation \n",
    "\n",
    "scores = cross_val_score(cv_model, features, incidence_y, cv=cv)\n",
    "\n",
    "print(scores)  # accuracies\n",
    "print(scores.mean())\n",
    "print(scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.97070001  0.96960002  0.97720001  0.97000002  0.97500001]\n",
      "0.972500012875\n",
      "0.00304104780247\n"
     ]
    }
   ],
   "source": [
    "print(scores)  # accuracies\n",
    "print(scores.mean())\n",
    "print(scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.98270001  0.98100002  0.98490001  0.98050002  0.98290001  <br/>\n",
    "0.982400013161 <br/>\n",
    "0.00155948217113"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ 0.889  0.91   0.911  0.913  0.911] <br/>\n",
    "0.9068<br/>\n",
    "0.00895321171424\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
