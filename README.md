# About Deep Neural Network
* Informally, there are functions you can compute with a small L-layer deep neural network (i.e. small number of unit per layer) that shallower networks require exponentially more hidden units to compute. Take for example the XOR gate function, 



# Learning Machine Learning
Implementations of different algorithms in Machine learning and Deep Learning found while I am teaching myself in different courses and tutorials
* http://colah.github.io/posts/2015-08-Backprop/
* https://research.fb.com/facebook-ai-academy/ 
* http://www.cs.cmu.edu/~ninamf/courses/601sp15/lectures.shtml
* https://www.infoq.com/presentations/machine-learning-general-programming
* http://www.r2d3.us/visual-intro-to-machine-learning-part-1/

## Tips and Tricks

### Overfitting
* You can solve overfitting by:
  * Reduce number of features (either manually or by using Model Selection Algorithms), or
  * Regularization

### Regularization
* Regularization work well when we have a lot of features, each of which contributes a bit to predicting y.
* It tries to obtain “Simpler” hypothesis, by reducing parameters values (theta), which will result in less prone to overfitting. 

